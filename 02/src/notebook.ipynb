{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2: Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = None\n",
    "\n",
    "with open('./../data/data.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/wiedzmin/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to /home/wiedzmin/nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/wiedzmin/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/wiedzmin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/wiedzmin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/wiedzmin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/wiedzmin/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/wiedzmin/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/wiedzmin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to /home/wiedzmin/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "# Download Corpora -> brown webtext words stopwords\n",
    "# Download Models -> punkt averaged_perceptron_tagger maxent_ne_chunker vader_lexicon wordnet tagsets\n",
    "nltk.download([\"brown\",\"webtext\", \"words\", \"stopwords\"] )\n",
    "nltk.download([\"punkt\", \"averaged_perceptron_tagger\", \"maxent_ne_chunker\", \"vader_lexicon\", \"wordnet\", \"tagsets\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def tokenCounts(tokens):\n",
    "    counts = Counter(tokens)\n",
    "    sortedCounts = sorted(counts.items(), key=lambda count:count[1], reverse=True)\n",
    "    return sortedCounts\n",
    "\n",
    "def listCounts(text, lst):\n",
    "    res = []\n",
    "    for item in lst:\n",
    "        item = item + (text.count(item[0]),)\n",
    "        item = [item[0], item[1], item[2]]\n",
    "        res.append(item)\n",
    "    res = sorted(res, key=lambda item: item[2], reverse=True)\n",
    "    return res\n",
    "\n",
    "def dictCounts(text, d):\n",
    "    items = d.items()\n",
    "    res = []\n",
    "    for item in items:\n",
    "        item = item + (text.count(item[0]),)\n",
    "        item = [item[0], item[1], item[2]]\n",
    "        res.append(item)\n",
    "    res = sorted(res, key=lambda item: item[2], reverse=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words total: 12781\n",
      "[(',', 714), ('the', 643), ('.', 533), ('of', 333), ('to', 330), ('and', 268), ('in', 218), ('a', 208), ('that', 149), ('is', 143), (\"'s\", 112), (\"''\", 97), ('are', 93), ('it', 93), ('``', 92), ('for', 90), ('says', 84), ('be', 73), ('The', 73), ('as', 72)]\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(text)\n",
    "print('Words total: {}'.format(len(tokens)))\n",
    "print(tokenCounts(tokens)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter punctation and stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered words cnt: 6741\n",
      "[(\"'s\", 112), ('says', 84), ('The', 73), ('geothermal', 41), ('Ireland', 36), ('sargassum', 36), ('land', 33), ('ground', 32), ('In', 31), ('years', 28), ('also', 27), ('Halligen', 27), ('steam', 26), ('permafrost', 26), ('But', 24), ('Olkaria', 24), ('could', 24), ('one', 23), ('It', 23), (\"n't\", 22), ('water', 22), ('would', 21), ('people', 20), ('sea', 20), ('climate', 19), ('heat', 18), ('power', 18), ('We', 18), ('energy', 17), ('much', 17)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "def filterTokens(t):\n",
    "    # Filter custom special characters from text\n",
    "    extra_punctuation = punctuation + \"–''``’“\"\n",
    "    stops = stopwords.words('english')\n",
    "    filtered_tokens = [token for token in t if token not in extra_punctuation]\n",
    "    filtered_tokens = [token for token in filtered_tokens if token not in stops]\n",
    "    return filtered_tokens\n",
    "\n",
    "filtered_tokens = filterTokens(tokens)\n",
    "print('Filtered words cnt: {}'.format(len(filtered_tokens)))\n",
    "print(tokenCounts(filtered_tokens)[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus.reader.wordnet import NOUN,VERB\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# lemmas = {token:lemmatizer.lemmatize(token, pos=VERB) for token in filtered_tokens}\n",
    "# print('Lemmas count: {}'.format(len(lemmas)))\n",
    "\n",
    "# tf = {}\n",
    "\n",
    "# for key, val in lemmas.items():\n",
    "#     print([key, val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST results cnt: 6741\n",
      "Top 50 POS results:\n",
      "[[\"'s\", 'POS', 112], [\"'s\", 'POS', 112], [\"'s\", 'POS', 112], ['In', 'IN', 43], ['D', 'NNP', 38], ['steam', 'NN', 28], ['road', 'NN', 22], ['energy', 'NN', 18], ['see', 'VB', 18], ['Kenya', 'NNP', 17], ['past', 'IN', 13], ['region', 'NN', 12], ['Africa', 'NNP', 7], ['tectonic', 'JJ', 7], ['sometimes', 'RB', 7], ['clean', 'VBP', 6], ['Hell', 'NNP', 6], ['National', 'NNP', 6], ['Rift', 'NNP', 5], ['apart', 'RB', 5], ['Gate', 'NNP', 5], ['East', 'NNP', 4], ['Great', 'NNP', 4], ['Valley', 'NNP', 4], ['continent', 'JJ', 4], ['along', 'IN', 4], ['must', 'MD', 4], ['releasing', 'VBG', 3], ['winds', 'VBZ', 3], ['park', 'NN', 3], ['volcanic', 'JJ', 2], ['shifts', 'NNS', 2], ['quantities', 'NNS', 2], ['giraffes', 'VBP', 2], [\"'ll\", 'MD', 2], ['avoid', 'JJ', 2], ['tearing', 'VBG', 1], ['unimaginable', 'JJ', 1], ['Drive', 'NNP', 1], ['dusty', 'JJ', 1], ['dirt', 'NN', 1], ['zebra', 'NN', 1], ['gazelles', 'NNS', 1], ['plume', 'JJ', 1], ['shooting', 'VBG', 1], ['skyward', 'JJ', 1], ['distance', 'NN', 1], ['Vehicles', 'NNP', 1], ['swerve', 'VB', 1], ['running', 'VBG', 1]]\n"
     ]
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(filtered_tokens)\n",
    "print('POST results cnt: {}'.format(len(tagged)))\n",
    "print('Top 50 POS results:')\n",
    "print(listCounts(text, tagged[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER with entity classification (using nltk.ne_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW binary results cnt: 184\n",
      "Top 50 NER binary entities:\n",
      "[['Ireland', 'NE', 36], ['Hallig', 'NE', 32], ['Halligen', 'NE', 27], ['Olkaria', 'NE', 24], ['Kenya', 'NE', 17], ['Earth', 'NE', 13], ['Atlantic', 'NE', 12], ['German', 'NE', 11], ['Newson', 'NE', 10], ['Karingithi', 'NE', 9], ['Daltun', 'NE', 9], ['Hooge', 'NE', 9], ['Mexico', 'NE', 9], ['Mwangi', 'NE', 8], ['County', 'NE', 8], ['Germany', 'NE', 8], ['Nordstrandischmoor', 'NE', 8], ['Africa', 'NE', 7], ['Arctic', 'NE', 7], ['Hell', 'NE', 6], ['Maasai', 'NE', 6], ['Iceland', 'NE', 6], ['Tibet', 'NE', 6], ['Doré', 'NE', 6], ['Irish', 'NE', 6], ['North Sea', 'NE', 6], ['Deicke', 'NE', 6], ['Morrison', 'NE', 6], ['Olkaria V', 'NE', 5], ['CO2', 'NE', 5], ['Northern Ireland', 'NE', 5], ['Fogarty', 'NE', 5], ['Rösner', 'NE', 5], ['Caribbean', 'NE', 5], ['Wang', 'NE', 5], ['Olkaria VI', 'NE', 4], ['KenGen', 'NE', 4], ['Geothermal', 'NE', 4], ['Rift Valley', 'NE', 4], ['Nyaga', 'NE', 4], ['Canada', 'NE', 4], ['Highway', 'NE', 4], ['Hansen', 'NE', 4], ['Wadden Sea', 'NE', 4], ['Gröde', 'NE', 4], ['East Africa', 'NE', 3], ['Great Rift Valley', 'NE', 3], ['Anna Mwangi', 'NE', 3], ['Suswa', 'NE', 3], ['Sempui', 'NE', 3]]\n",
      "NER results cnt: 218\n",
      "Top 50 NER results:\n",
      "[['Ireland', 'GPE', 36], ['Hallig', 'PERSON', 32], ['Halligen', 'PERSON', 27], ['Olkaria', 'GPE', 24], ['Kenya', 'PERSON', 17], ['Earth', 'PERSON', 13], ['Atlantic', 'ORGANIZATION', 12], ['German', 'GPE', 11], ['Newson', 'ORGANIZATION', 10], ['Karingithi', 'PERSON', 9], ['Mexico', 'GPE', 9], ['Daltun', 'PERSON', 9], ['Hooge', 'PERSON', 9], ['Mwangi', 'PERSON', 8], ['All', 'PERSON', 8], ['Germany', 'GPE', 8], ['Nordstrandischmoor', 'GPE', 8], ['Africa', 'PERSON', 7], ['Arctic', 'ORGANIZATION', 7], ['Reid', 'PERSON', 7], ['Hell', 'PERSON', 6], ['Maasai', 'PERSON', 6], ['Iceland', 'GPE', 6], ['Doré', 'PERSON', 6], ['Irish', 'GPE', 6], ['North Sea', 'LOCATION', 6], ['Deicke', 'PERSON', 6], ['Allen', 'PERSON', 6], ['Morrison', 'PERSON', 6], ['Gate', 'ORGANIZATION', 5], ['Olkaria V', 'PERSON', 5], ['CO2', 'ORGANIZATION', 5], ['Northern Ireland', 'GPE', 5], ['Fogarty', 'PERSON', 5], ['Rösner', 'PERSON', 5], ['Caribbean', 'LOCATION', 5], ['Wang', 'PERSON', 5], ['Olkaria VI', 'PERSON', 4], ['KenGen', 'ORGANIZATION', 4], ['Geothermal', 'ORGANIZATION', 4], ['Rift Valley', 'PERSON', 4], ['Nyaga', 'PERSON', 4], ['Canada', 'GPE', 4], ['Highway', 'PERSON', 4], ['Hansen', 'PERSON', 4], ['Wadden Sea', 'PERSON', 4], ['Gröde', 'GPE', 4], ['East Africa', 'PERSON', 3], ['Great Rift Valley', 'PERSON', 3], ['Anna Mwangi', 'PERSON', 3]]\n"
     ]
    }
   ],
   "source": [
    "ne_chunked_binary = nltk.ne_chunk(tagged, binary=True)\n",
    "ne_chunked = nltk.ne_chunk(tagged, binary=False)\n",
    "\n",
    "def extractEntities(ne_chunked):\n",
    "    data = {}\n",
    "    for entity in ne_chunked:\n",
    "        if isinstance(entity, nltk.tree.Tree):\n",
    "            text = \" \".join([word for word, tag in entity.leaves()])\n",
    "            ent = entity.label()\n",
    "            data[text] = ent\n",
    "        else:\n",
    "            continue\n",
    "    return data\n",
    "\n",
    "ne_binary = extractEntities(ne_chunked_binary)\n",
    "ne_binary_cnts = dictCounts(text, ne_binary)\n",
    "print('NEW binary results cnt: {}'.format(len(ne_binary_cnts)))\n",
    "print('Top 50 NER binary entities:')\n",
    "print(ne_binary_cnts[:50])\n",
    "\n",
    "ne = extractEntities(ne_chunked)\n",
    "ne_cnts = dictCounts(text, ne)\n",
    "print('NER results cnt: {}'.format(len(ne_cnts)))\n",
    "print('Top 50 NER results:')\n",
    "print(ne_cnts[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER with custom patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER custom results cnt: 303\n",
      "NER custom top results:\n",
      "[['Ireland', 'NP', 36], ['Hallig', 'NP', 32], ['Halligen', 'NP', 27], ['Olkaria', 'NP', 24], ['Kenya', 'NP', 17], ['As', 'NP', 17], ['Earth', 'NP', 13], ['Atlantic', 'NP', 12], ['Newson', 'NP', 10], ['Hartwig-Kruse', 'NP', 10], ['Karingithi', 'NP', 9], ['Daltun', 'NP', 9], ['Hooge', 'NP', 9], ['Mexico', 'NP', 9], ['Mwangi', 'NP', 8], ['All', 'NP', 8], ['Germany', 'NP', 8], ['Nordstrandischmoor', 'NP', 8], ['Africa', 'NP', 7], ['Arctic', 'NP', 7], ['Reid', 'NP', 7], ['Hell', 'NP', 6], ['Maasai', 'NP', 6], ['Iceland', 'NP', 6], ['Doré', 'NP', 6], ['Republic', 'NP', 6], ['North Sea', 'NP', 6], ['Deicke', 'NP', 6], ['Allen', 'NP', 6], ['Morrison', 'NP', 6], ['Gate', 'NP', 5], ['Olkaria V', 'NP', 5], ['CO2', 'NP', 5], ['Northern Ireland', 'NP', 5], ['Fogarty', 'NP', 5], [\"O'Connell\", 'NP', 5], ['Rösner', 'NP', 5], ['Caribbean', 'NP', 5], ['Wang', 'NP', 5], ['Olkaria VI', 'NP', 4], ['KenGen', 'NP', 4], ['Geothermal', 'NP', 4], ['Rift Valley', 'NP', 4], ['Nyaga', 'NP', 4], ['Canada', 'NP', 4], ['Hansen', 'NP', 4], ['Europe', 'NP', 4], ['Wadden Sea', 'NP', 4], ['Gröde', 'NP', 4], ['East Africa', 'NP', 3]]\n"
     ]
    }
   ],
   "source": [
    "# print(tagged)\n",
    "grammar = \"NP: {<DT>?<JJ>*<NNP|NNPS>+}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "ne_custom_tmp = extractEntities(cp.parse(tagged))\n",
    "ne_custom = {}\n",
    "for entity, phr in ne_custom_tmp.items():\n",
    "    if len(entity) > 1:\n",
    "        ne_custom[entity] = phr \n",
    "print('NER custom results cnt: {}'.format(len(ne_custom)))\n",
    "print('NER custom top results:')\n",
    "# print(dictCounts(text, ne_custom)[:50])\n",
    "ne_custom_cnts = dictCounts(text, ne_custom)\n",
    "print(ne_custom_cnts[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom entity classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER using nltk.ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Ireland', 'GPE', 36], 'piece of subcontinental land']\n",
      "[['Hallig', 'PERSON', 32], 'small islands without protective dikes']\n",
      "[['Halligen', 'PERSON', 27], 'small islands without protective dikes']\n",
      "[['Olkaria', 'GPE', 24], 'region']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wiedzmin/.local/lib/python3.8/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/wiedzmin/.local/lib/python3.8/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Kenya', 'PERSON', 17], 'generic']\n",
      "[['Earth', 'PERSON', 13], 'generic']\n",
      "[['Atlantic', 'ORGANIZATION', 12], 'second-largest of the world']\n",
      "[['German', 'GPE', 11], 'country at the intersection']\n",
      "[['Newson', 'ORGANIZATION', 10], 'generic']\n",
      "[['Karingithi', 'PERSON', 9], 'generic']\n",
      "[['Mexico', 'GPE', 9], 'country in the southern portion']\n",
      "[['Daltun', 'PERSON', 9], 'pair of agreements']\n",
      "[['Hooge', 'PERSON', 9], 'generic']\n",
      "[['Mwangi', 'PERSON', 8], 'Kenyan photojournalist']\n",
      "[['All', 'PERSON', 8], 'sorosilicate group of minerals']\n",
      "[['Germany', 'GPE', 8], 'country at the intersection']\n",
      "[['Nordstrandischmoor', 'GPE', 8], 'generic']\n",
      "[['Africa', 'PERSON', 7], 'country']\n",
      "[['Arctic', 'ORGANIZATION', 7], 'generic']\n",
      "[['Reid', 'PERSON', 7], 'technique wherein']\n",
      "[['Hell', 'PERSON', 6], 'generic']\n",
      "[['Maasai', 'PERSON', 6], 'Nilotic ethnic group']\n",
      "[['Iceland', 'GPE', 6], 'island in the North Atlantic']\n",
      "[['Doré', 'PERSON', 6], 'generic']\n",
      "[['Irish', 'GPE', 6], 'islands of Ireland and Great Britain']\n",
      "[['North Sea', 'LOCATION', 6], 'sea of the Atlantic Ocean']\n",
      "[['Deicke', 'PERSON', 6], 'generic']\n",
      "[['Allen', 'PERSON', 6], 'English actor']\n",
      "[['Morrison', 'PERSON', 6], 'fourth largest chain of supermarkets']\n",
      "[['Gate', 'ORGANIZATION', 5], 'structured form of play']\n",
      "[['Olkaria V', 'PERSON', 5], 'power station in Kenya']\n",
      "[['CO2', 'ORGANIZATION', 5], 'colorless gas with a density']\n",
      "[['Northern Ireland', 'GPE', 5], 'generic']\n",
      "[['Fogarty', 'PERSON', 5], 'surname of Irish origin']\n",
      "[['Rösner', 'PERSON', 5], 'hostage-taking crisis']\n",
      "[['Caribbean', 'LOCATION', 5], 'region of the Americas']\n",
      "[['Wang', 'PERSON', 5], 'type of fin']\n",
      "[['Olkaria VI', 'PERSON', 4], 'geothermal power station in Kenya']\n",
      "[['KenGen', 'ORGANIZATION', 4], 'parastatal company']\n",
      "[['Geothermal', 'ORGANIZATION', 4], 'thermal energy']\n",
      "[['Rift Valley', 'PERSON', 4], 'linear shaped lowland between several highlands']\n",
      "[['Nyaga', 'PERSON', 4], 'generic']\n",
      "[['Canada', 'GPE', 4], 'country in the northern part']\n",
      "[['Highway', 'PERSON', 4], 'public or private road']\n",
      "[['Hansen', 'PERSON', 4], 'American beverage company']\n",
      "[['Wadden Sea', 'PERSON', 4], 'intertidal zone in the southeastern part']\n",
      "[['Gröde', 'GPE', 4], 'municipality in the district']\n",
      "[['East Africa', 'PERSON', 3], 'eastern subregion of the African continent']\n",
      "[['Great Rift Valley', 'PERSON', 3], 'series of contiguous geographic trenches']\n",
      "[['Anna Mwangi', 'PERSON', 3], 'Kenyan-Norwegian singer']\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def nearestTerm(term, options):\n",
    "    maxRatio = -1.0\n",
    "    nearest = options[0]\n",
    "    for opt in options:\n",
    "        # Use longest contiguous matching subsequence\n",
    "        ratio = SequenceMatcher(None, term, opt).ratio()\n",
    "        if ratio > maxRatio:\n",
    "            maxRatio = ratio\n",
    "            nearest = opt\n",
    "    return nearest\n",
    "\n",
    "def classFromSummary(summary, generic):\n",
    "    first_sent = nltk.sent_tokenize(summary)[0]\n",
    "    first_sent_tokens = nltk.word_tokenize(first_sent)\n",
    "    first_sent_tagged = nltk.pos_tag(first_sent_tokens)\n",
    "#     print(first_sent)\n",
    "#     print(first_sent_tagged)\n",
    "    grammar = \"NP: {<VBZ|VBP|VBD><DT><JJ|JJR|JJS|CC|NNP|IN|,>*<NN|NNS>+(<IN><DT>?<JJ|JJR|JJS|CC|NNP|IN|,>*<NN|NNS|NNP|NNPS>+)?}\"\n",
    "    c = nltk.RegexpParser(grammar)\n",
    "    classification = extractEntities(c.parse(first_sent_tagged))\n",
    "    if len(classification) < 1:\n",
    "        grammar = \"NP: {<VBZ|VBP|VBD><DT>?<JJ|JJR|JJS|CC|NNP|IN|,>*<NN|NNS>+(<IN><DT>?<JJ|JJR|JJS|CC|NNP|IN|,>*<NN|NNS|NNP|NNPS>+)?}\"\n",
    "        c = nltk.RegexpParser(grammar)\n",
    "        classification = extractEntities(c.parse(first_sent_tagged))\n",
    "        if len(classification) < 1:\n",
    "            return generic\n",
    "    classification = next(iter(classification))\n",
    "#     print(classification)\n",
    "    classification_tagged = nltk.pos_tag(nltk.word_tokenize(classification))\n",
    "    grammar = \"NP: {<JJ|JJR|JJS|CC|NNP|IN|,>*<NN|NNS>+(<IN><DT>?<JJ|JJR|JJS|CC|NNP|IN|,>*<NN|NNS|NNP|NNPS>+)?}\"\n",
    "    c = nltk.RegexpParser(grammar)\n",
    "    classification = extractEntities(c.parse(classification_tagged))\n",
    "    classification = next(iter(classification))\n",
    "    return classification\n",
    "\n",
    "def wikipediaClassification(entity):\n",
    "    generic_class = 'generic'\n",
    "    try:\n",
    "        results = wikipedia.search(entity)\n",
    "        if len(results) < 1:\n",
    "            return generic_class\n",
    "        page = wikipedia.page(results[0])\n",
    "        return classFromSummary(page.summary, generic_class)\n",
    "    except wikipedia.DisambiguationError as e:\n",
    "        # Handle ambiguous search result - find nearest option\n",
    "        nearest = nearestTerm(entity, e.options)\n",
    "        try:\n",
    "            page = wikipedia.page(nearest)\n",
    "            return classFromSummary(page.summary, generic_class)\n",
    "        except (wikipedia.DisambiguationError, wikipedia.PageError):\n",
    "            # In case of multiple ambiguousity, return generic classification\n",
    "            return generic_class\n",
    "    except wikipedia.PageError:\n",
    "        return generic_class\n",
    "\n",
    "for entity in ne_cnts[:50]:\n",
    "    classification = wikipediaClassification(entity[0])\n",
    "    print([entity, classification])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER with custom patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Ireland', 'NP', 36], 'piece of subcontinental land']\n",
      "[['Hallig', 'NP', 32], 'small islands without protective dikes']\n",
      "[['Halligen', 'NP', 27], 'small islands without protective dikes']\n",
      "[['Olkaria', 'NP', 24], 'region']\n",
      "[['Kenya', 'NP', 17], 'generic']\n",
      "[['As', 'NP', 17], 'first letter']\n",
      "[['Earth', 'NP', 13], 'generic']\n",
      "[['Atlantic', 'NP', 12], 'second-largest of the world']\n",
      "[['Newson', 'NP', 10], 'generic']\n",
      "[['Hartwig-Kruse', 'NP', 10], 'German politician for the populist Alternative for Germany']\n",
      "[['Karingithi', 'NP', 9], 'generic']\n",
      "[['Daltun', 'NP', 9], 'pair of agreements']\n",
      "[['Hooge', 'NP', 9], 'generic']\n",
      "[['Mexico', 'NP', 9], 'country in the southern portion']\n",
      "[['Mwangi', 'NP', 8], 'Kenyan photojournalist']\n",
      "[['All', 'NP', 8], 'sorosilicate group of minerals']\n",
      "[['Germany', 'NP', 8], 'country at the intersection']\n",
      "[['Nordstrandischmoor', 'NP', 8], 'generic']\n",
      "[['Africa', 'NP', 7], 'country']\n",
      "[['Arctic', 'NP', 7], 'generic']\n",
      "[['Reid', 'NP', 7], 'technique wherein']\n",
      "[['Hell', 'NP', 6], 'generic']\n",
      "[['Maasai', 'NP', 6], 'Nilotic ethnic group']\n",
      "[['Iceland', 'NP', 6], 'island in the North Atlantic']\n",
      "[['Doré', 'NP', 6], 'generic']\n",
      "[['Republic', 'NP', 6], 'form of government']\n",
      "[['North Sea', 'NP', 6], 'sea of the Atlantic Ocean']\n",
      "[['Deicke', 'NP', 6], 'generic']\n",
      "[['Allen', 'NP', 6], 'English actor']\n",
      "[['Morrison', 'NP', 6], 'fourth largest chain of supermarkets']\n",
      "[['Gate', 'NP', 5], 'structured form of play']\n",
      "[['Olkaria V', 'NP', 5], 'power station in Kenya']\n",
      "[['CO2', 'NP', 5], 'colorless gas with a density']\n",
      "[['Northern Ireland', 'NP', 5], 'generic']\n",
      "[['Fogarty', 'NP', 5], 'surname of Irish origin']\n",
      "[[\"O'Connell\", 'NP', 5], 'American singer-songwriter']\n",
      "[['Rösner', 'NP', 5], 'hostage-taking crisis']\n",
      "[['Caribbean', 'NP', 5], 'region of the Americas']\n",
      "[['Wang', 'NP', 5], 'type of fin']\n",
      "[['Olkaria VI', 'NP', 4], 'geothermal power station in Kenya']\n",
      "[['KenGen', 'NP', 4], 'parastatal company']\n",
      "[['Geothermal', 'NP', 4], 'thermal energy']\n",
      "[['Rift Valley', 'NP', 4], 'linear shaped lowland between several highlands']\n",
      "[['Nyaga', 'NP', 4], 'generic']\n",
      "[['Canada', 'NP', 4], 'country in the northern part']\n",
      "[['Hansen', 'NP', 4], 'American beverage company']\n",
      "[['Europe', 'NP', 4], 'continent']\n",
      "[['Wadden Sea', 'NP', 4], 'intertidal zone in the southeastern part']\n",
      "[['Gröde', 'NP', 4], 'municipality in the district']\n",
      "[['East Africa', 'NP', 3], 'eastern subregion of the African continent']\n"
     ]
    }
   ],
   "source": [
    "for entity in ne_custom_cnts[:50]:\n",
    "    classification = wikipediaClassification(entity[0])\n",
    "    print([entity, classification])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
