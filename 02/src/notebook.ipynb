{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2: Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = None\n",
    "\n",
    "with open('./../data/data.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/wiedzmin/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to /home/wiedzmin/nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/wiedzmin/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/wiedzmin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/wiedzmin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/wiedzmin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/wiedzmin/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/wiedzmin/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/wiedzmin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to /home/wiedzmin/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "# Download Corpora -> brown webtext words stopwords\n",
    "# Download Models -> punkt averaged_perceptron_tagger maxent_ne_chunker vader_lexicon wordnet tagsets\n",
    "nltk.download([\"brown\",\"webtext\", \"words\", \"stopwords\"] )\n",
    "nltk.download([\"punkt\", \"averaged_perceptron_tagger\", \"maxent_ne_chunker\", \"vader_lexicon\", \"wordnet\", \"tagsets\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def tokenCounts(tokens):\n",
    "    counts = Counter(tokens)\n",
    "    sortedCounts = sorted(counts.items(), key=lambda count:count[1], reverse=True)\n",
    "    return sortedCounts\n",
    "\n",
    "def listCounts(terms, lst):\n",
    "    print(lst)\n",
    "#     res = {}\n",
    "#     for item in lst:\n",
    "#         item = item + (terms.count(item[0]),)\n",
    "#         item = [item[0], item[1], item[2]]\n",
    "#         try:\n",
    "#             res[item[0]][2] += item[2]\n",
    "#         except KeyError:\n",
    "#             res[item[0]] = item\n",
    "#     print(res)\n",
    "#     res = sorted(res.items(), key=lambda item: item[2], reverse=True)\n",
    "    return res\n",
    "\n",
    "def dictCounts(text, d):\n",
    "    items = d.items()\n",
    "    res = []\n",
    "    for item in items:\n",
    "        item = item + (text.count(item[0]),)\n",
    "        item = [item[0], item[1], item[2]]\n",
    "        res.append(item)\n",
    "    res = sorted(res, key=lambda item: item[2], reverse=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words total: 12728\n",
      "[(',', 714), ('the', 640), ('.', 533), ('of', 332), ('to', 330), ('and', 268), ('in', 218), ('a', 207), ('that', 149), ('is', 142), (\"'s\", 110), (\"''\", 97), ('are', 93), ('it', 93), ('``', 92), ('for', 90), ('says', 84), ('be', 73), ('as', 72), ('on', 71)]\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(text)\n",
    "print('Words total: {}'.format(len(tokens)))\n",
    "print(tokenCounts(tokens)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter punctation and stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered words cnt: 6280\n",
      "[(\"'s\", 110), ('says', 84), ('geothermal', 41), ('Ireland', 36), ('sargassum', 36), ('land', 33), ('ground', 32), ('years', 28), ('Halligen', 27), ('steam', 26), ('also', 26), ('permafrost', 26), ('Olkaria', 24), ('could', 24), ('one', 23), (\"n't\", 22), ('water', 22), ('would', 21), ('people', 20), ('sea', 20), ('climate', 19), ('heat', 18), ('power', 18), ('energy', 17), ('much', 17), ('many', 17), ('island', 17), ('”', 16), ('like', 15), ('mounds', 15)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "def filterTokens(t):\n",
    "    # Filter custom special characters from text\n",
    "    extra_punctuation = punctuation + \"–''``’“\"\n",
    "    stops = stopwords.words('english')\n",
    "    filtered_tokens = [token for token in t if token not in extra_punctuation]\n",
    "    filtered_tokens = [token for token in filtered_tokens if token.lower() not in stops]\n",
    "    return filtered_tokens\n",
    "\n",
    "filtered_tokens = filterTokens(tokens)\n",
    "print('Filtered words cnt: {}'.format(len(filtered_tokens)))\n",
    "print(tokenCounts(filtered_tokens)[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus.reader.wordnet import NOUN,VERB\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# lemmas = {token:lemmatizer.lemmatize(token, pos=VERB) for token in filtered_tokens}\n",
    "# print('Lemmas count: {}'.format(len(lemmas)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results total: 6280\n",
      "Top 50 results:\n",
      "((\"'s\", 'POS'), 109)\n",
      "(('says', 'VBZ'), 84)\n",
      "(('Ireland', 'NNP'), 36)\n",
      "(('geothermal', 'JJ'), 32)\n",
      "(('ground', 'NN'), 31)\n",
      "(('land', 'NN'), 29)\n",
      "(('years', 'NNS'), 28)\n",
      "(('Halligen', 'NNP'), 27)\n",
      "(('also', 'RB'), 26)\n",
      "(('Olkaria', 'NNP'), 24)\n",
      "(('could', 'MD'), 24)\n",
      "(('one', 'CD'), 23)\n",
      "((\"n't\", 'RB'), 22)\n",
      "(('water', 'NN'), 22)\n",
      "(('steam', 'NN'), 21)\n",
      "(('would', 'MD'), 21)\n",
      "(('people', 'NNS'), 20)\n",
      "(('sargassum', 'NN'), 19)\n",
      "(('power', 'NN'), 18)\n",
      "(('energy', 'NN'), 17)\n",
      "(('many', 'JJ'), 17)\n",
      "(('permafrost', 'NN'), 17)\n",
      "(('heat', 'NN'), 16)\n",
      "(('sea', 'NN'), 16)\n",
      "(('climate', 'NN'), 15)\n",
      "(('Kenya', 'NNP'), 14)\n",
      "(('way', 'NN'), 14)\n",
      "(('year', 'NN'), 14)\n",
      "(('floods', 'NNS'), 14)\n",
      "(('sargassum', 'JJ'), 14)\n",
      "(('road', 'NN'), 13)\n",
      "(('Earth', 'NNP'), 13)\n",
      "(('world', 'NN'), 13)\n",
      "(('well', 'RB'), 13)\n",
      "(('like', 'IN'), 13)\n",
      "(('large', 'JJ'), 13)\n",
      "(('island', 'NN'), 13)\n",
      "(('mounds', 'NNS'), 13)\n",
      "(('small', 'JJ'), 12)\n",
      "(('damage', 'NN'), 12)\n",
      "(('church', 'NN'), 12)\n",
      "(('rewilding', 'VBG'), 12)\n",
      "(('even', 'RB'), 11)\n",
      "(('new', 'JJ'), 11)\n",
      "(('much', 'JJ'), 11)\n",
      "(('keep', 'VB'), 11)\n",
      "(('Atlantic', 'NNP'), 11)\n",
      "(('Sea', 'NNP'), 11)\n",
      "(('miles', 'NNS'), 10)\n",
      "(('still', 'RB'), 10)\n"
     ]
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(filtered_tokens)\n",
    "print('Results total: {}'.format(len(tagged)))\n",
    "print('Top 50 results:')\n",
    "for item in tokenCounts(tagged)[:50]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER with entity classification (using nltk.ne_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary results total: 179\n",
      "Top 50 binary results:\n",
      "['Ireland', 'NE', 36]\n",
      "['Halligen', 'NE', 27]\n",
      "['Olkaria', 'NE', 24]\n",
      "['Kenya', 'NE', 14]\n",
      "['Earth', 'NE', 13]\n",
      "['Atlantic', 'NE', 11]\n",
      "['Newson', 'NE', 10]\n",
      "['Karingithi', 'NE', 9]\n",
      "['Daltun', 'NE', 9]\n",
      "['Hooge', 'NE', 9]\n",
      "['Mexico', 'NE', 9]\n",
      "['Mwangi', 'NE', 8]\n",
      "['County', 'NE', 8]\n",
      "['Germany', 'NE', 8]\n",
      "['Nordstrandischmoor', 'NE', 8]\n",
      "['Africa', 'NE', 7]\n",
      "['Reid', 'NE', 7]\n",
      "['Maasai', 'NE', 6]\n",
      "['Iceland', 'NE', 6]\n",
      "['Arctic', 'NE', 6]\n",
      "['Doré', 'NE', 6]\n",
      "['Irish', 'NE', 6]\n",
      "['Deicke', 'NE', 6]\n",
      "['Allen', 'NE', 6]\n",
      "['Morrison', 'NE', 6]\n",
      "['Hell', 'NE', 5]\n",
      "['CO2', 'NE', 5]\n",
      "['Fogarty', 'NE', 5]\n",
      "['Hallig', 'NE', 5]\n",
      "['Rösner', 'NE', 5]\n",
      "['Caribbean', 'NE', 5]\n",
      "['Wang', 'NE', 5]\n",
      "['KenGen', 'NE', 4]\n",
      "['Geothermal', 'NE', 4]\n",
      "['Nyaga', 'NE', 4]\n",
      "['Canada', 'NE', 4]\n",
      "['Highway', 'NE', 4]\n",
      "['Hansen', 'NE', 4]\n",
      "['Europe', 'NE', 4]\n",
      "['Gröde', 'NE', 4]\n",
      "['Suswa', 'NE', 3]\n",
      "['Sempui', 'NE', 3]\n",
      "['China', 'NE', 3]\n",
      "['Russian', 'NE', 3]\n",
      "['Eoghan', 'NE', 3]\n",
      "['Friers', 'NE', 3]\n",
      "['Bergin', 'NE', 3]\n",
      "['German', 'NE', 3]\n",
      "['Ruth', 'NE', 3]\n",
      "['Kolk', 'NE', 3]\n",
      "Results total: 209\n",
      "Top 50 results:\n",
      "['Ireland', 'GPE', 36]\n",
      "['Halligen', 'PERSON', 27]\n",
      "['Olkaria', 'GPE', 24]\n",
      "['Kenya', 'PERSON', 14]\n",
      "['Earth', 'PERSON', 13]\n",
      "['Atlantic', 'ORGANIZATION', 11]\n",
      "['Newson', 'ORGANIZATION', 10]\n",
      "['Karingithi', 'PERSON', 9]\n",
      "['Mexico', 'GPE', 9]\n",
      "['Daltun', 'PERSON', 9]\n",
      "['Hooge', 'PERSON', 9]\n",
      "['Mwangi', 'PERSON', 8]\n",
      "['County', 'PERSON', 8]\n",
      "['Germany', 'GPE', 8]\n",
      "['Nordstrandischmoor', 'GPE', 8]\n",
      "['Africa', 'PERSON', 7]\n",
      "['Reid', 'PERSON', 7]\n",
      "['Maasai', 'PERSON', 6]\n",
      "['Iceland', 'GPE', 6]\n",
      "['Arctic', 'ORGANIZATION', 6]\n",
      "['Doré', 'PERSON', 6]\n",
      "['Irish', 'GPE', 6]\n",
      "['Deicke', 'PERSON', 6]\n",
      "['Allen', 'PERSON', 6]\n",
      "['Morrison', 'PERSON', 6]\n",
      "['Hell', 'PERSON', 5]\n",
      "['Gate', 'ORGANIZATION', 5]\n",
      "['CO2', 'ORGANIZATION', 5]\n",
      "['Fogarty', 'PERSON', 5]\n",
      "['Hallig', 'PERSON', 5]\n",
      "['Rösner', 'PERSON', 5]\n",
      "['Caribbean', 'LOCATION', 5]\n",
      "['Wang', 'PERSON', 5]\n",
      "['KenGen', 'ORGANIZATION', 4]\n",
      "['Geothermal', 'ORGANIZATION', 4]\n",
      "['Nyaga', 'PERSON', 4]\n",
      "['Canada', 'GPE', 4]\n",
      "['Highway', 'PERSON', 4]\n",
      "['Hansen', 'PERSON', 4]\n",
      "['Europe', 'PERSON', 4]\n",
      "['Gröde', 'PERSON', 4]\n",
      "['Suswa', 'PERSON', 3]\n",
      "['Sempui', 'PERSON', 3]\n",
      "['China', 'GPE', 3]\n",
      "['Russian', 'GPE', 3]\n",
      "['Friers', 'ORGANIZATION', 3]\n",
      "['Bergin', 'PERSON', 3]\n",
      "['German', 'GPE', 3]\n",
      "['Ruth', 'PERSON', 3]\n",
      "['Kolk', 'PERSON', 3]\n"
     ]
    }
   ],
   "source": [
    "ne_chunked_binary = nltk.ne_chunk(tagged, binary=True)\n",
    "ne_chunked = nltk.ne_chunk(tagged, binary=False)\n",
    "\n",
    "def extractEntities(ne_chunked):\n",
    "    data = {}\n",
    "    for entity in ne_chunked:\n",
    "        if isinstance(entity, nltk.tree.Tree):\n",
    "            text = \" \".join([word for word, tag in entity.leaves()])\n",
    "            ent = entity.label()\n",
    "            data[text] = ent\n",
    "        else:\n",
    "            continue\n",
    "    return data\n",
    "\n",
    "ne_binary = extractEntities(ne_chunked_binary)\n",
    "ne_binary_cnts = dictCounts(filtered_tokens, ne_binary)\n",
    "print('Binary results total: {}'.format(len(ne_binary_cnts)))\n",
    "print('Top 50 binary results:')\n",
    "for item in ne_binary_cnts[:50]:\n",
    "    print(item)\n",
    "\n",
    "ne = extractEntities(ne_chunked)\n",
    "ne_cnts = dictCounts(filtered_tokens, ne)\n",
    "print('Results total: {}'.format(len(ne_cnts)))\n",
    "print('Top 50 results:')\n",
    "for item in ne_cnts[:50]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER with custom patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results total: 288\n",
      "Top 50 results:\n",
      "['Ireland', 'NP', 36]\n",
      "['Halligen', 'NP', 27]\n",
      "['Olkaria', 'NP', 24]\n",
      "['Kenya', 'NP', 14]\n",
      "['Earth', 'NP', 13]\n",
      "['Atlantic', 'NP', 11]\n",
      "['Newson', 'NP', 10]\n",
      "['Hartwig-Kruse', 'NP', 10]\n",
      "['Karingithi', 'NP', 9]\n",
      "['Daltun', 'NP', 9]\n",
      "['Hooge', 'NP', 9]\n",
      "['Mexico', 'NP', 9]\n",
      "['Mwangi', 'NP', 8]\n",
      "['Germany', 'NP', 8]\n",
      "['Nordstrandischmoor', 'NP', 8]\n",
      "['Africa', 'NP', 7]\n",
      "['Reid', 'NP', 7]\n",
      "['Maasai', 'NP', 6]\n",
      "['Iceland', 'NP', 6]\n",
      "['Arctic', 'NP', 6]\n",
      "['Doré', 'NP', 6]\n",
      "['Republic', 'NP', 6]\n",
      "['Deicke', 'NP', 6]\n",
      "['Allen', 'NP', 6]\n",
      "['Morrison', 'NP', 6]\n",
      "['Hell', 'NP', 5]\n",
      "['Gate', 'NP', 5]\n",
      "['CO2', 'NP', 5]\n",
      "['Fogarty', 'NP', 5]\n",
      "[\"O'Connell\", 'NP', 5]\n",
      "['Hallig', 'NP', 5]\n",
      "['Rösner', 'NP', 5]\n",
      "['Caribbean', 'NP', 5]\n",
      "['Wang', 'NP', 5]\n",
      "['KenGen', 'NP', 4]\n",
      "['Geothermal', 'NP', 4]\n",
      "['Nyaga', 'NP', 4]\n",
      "['Canada', 'NP', 4]\n",
      "['Hansen', 'NP', 4]\n",
      "['Europe', 'NP', 4]\n",
      "['Gröde', 'NP', 4]\n",
      "['Suswa', 'NP', 3]\n",
      "['Sempui', 'NP', 3]\n",
      "['China', 'NP', 3]\n",
      "['Friers', 'NP', 3]\n",
      "['Stout', 'NP', 3]\n",
      "['Bergin', 'NP', 3]\n",
      "['Schleswig-Holstein', 'NP', 3]\n",
      "['Without', 'NP', 3]\n",
      "['Kolk', 'NP', 3]\n"
     ]
    }
   ],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NNP|NNPS>+}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "ne_custom_tmp = extractEntities(cp.parse(tagged))\n",
    "ne_custom = {}\n",
    "for entity, phr in ne_custom_tmp.items():\n",
    "    if len(entity) > 1:\n",
    "        ne_custom[entity] = phr \n",
    "print('Results total: {}'.format(len(ne_custom)))\n",
    "print('Top 50 results:')\n",
    "ne_custom_cnts = dictCounts(filtered_tokens, ne_custom)\n",
    "for item in ne_custom_cnts[:50]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom entity classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER using nltk.ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Ireland', 'GPE', 36], 'piece of subcontinental land']\n",
      "[['Halligen', 'PERSON', 27], 'small islands without protective dikes']\n",
      "[['Olkaria', 'GPE', 24], 'lies']\n",
      "[['Kenya', 'PERSON', 14], 'generic']\n",
      "[['Earth', 'PERSON', 13], 'generic']\n",
      "[['Atlantic', 'ORGANIZATION', 11], \"second-largest of the world 's oceans\"]\n",
      "[['Newson', 'ORGANIZATION', 10], 'generic']\n",
      "[['Karingithi', 'PERSON', 9], 'generic']\n",
      "[['Mexico', 'GPE', 9], 'country in the southern portion of North America']\n",
      "[['Daltun', 'PERSON', 9], 'Goidelic language of the Insular Celtic branch']\n",
      "[['Hooge', 'PERSON', 9], 'generic']\n",
      "[['Mwangi', 'PERSON', 8], 'Kenyan photojournalist']\n",
      "[['County', 'PERSON', 8], 'distinct territorial body']\n",
      "[['Germany', 'GPE', 8], 'country at the intersection of Central']\n",
      "[['Nordstrandischmoor', 'GPE', 8], 'generic']\n",
      "[['Africa', 'PERSON', 7], 'country']\n",
      "[['Reid', 'PERSON', 7], 'technique wherein']\n",
      "[['Maasai', 'PERSON', 6], 'Nilotic ethnic group']\n",
      "[['Iceland', 'GPE', 6], 'island in the North Atlantic']\n",
      "[['Arctic', 'ORGANIZATION', 6], 'generic']\n",
      "[['Doré', 'PERSON', 6], 'generic']\n",
      "[['Irish', 'GPE', 6], 'islands of Ireland and Great Britain']\n",
      "[['Deicke', 'PERSON', 6], 'generic']\n",
      "[['Allen', 'PERSON', 6], 'English actor']\n",
      "[['Morrison', 'PERSON', 6], 'fourth largest chain of supermarkets']\n",
      "[['Hell', 'PERSON', 5], 'generic']\n",
      "[['Gate', 'ORGANIZATION', 5], 'structured form of play']\n",
      "[['CO2', 'ORGANIZATION', 5], 'colorless gas with a density']\n",
      "[['Fogarty', 'PERSON', 5], 'surname of Irish origin']\n",
      "[['Hallig', 'PERSON', 5], 'small islands without protective dikes']\n",
      "[['Rösner', 'PERSON', 5], 'hostage-taking crisis']\n",
      "[['Caribbean', 'LOCATION', 5], 'region of the Americas']\n",
      "[['Wang', 'PERSON', 5], 'type of fin']\n",
      "[['KenGen', 'ORGANIZATION', 4], 'parastatal company']\n",
      "[['Geothermal', 'ORGANIZATION', 4], 'thermal energy']\n",
      "[['Nyaga', 'PERSON', 4], 'generic']\n",
      "[['Canada', 'GPE', 4], 'country in the northern part of North America']\n",
      "[['Highway', 'PERSON', 4], 'public or private road']\n",
      "[['Hansen', 'PERSON', 4], 'American beverage company']\n",
      "[['Europe', 'PERSON', 4], 'continent']\n",
      "[['Gröde', 'PERSON', 4], 'municipality in the district of Nordfriesland']\n",
      "[['Suswa', 'PERSON', 3], 'shield volcano in the Great Rift Valley , Kenya']\n",
      "[['Sempui', 'PERSON', 3], 'generic']\n",
      "[['China', 'GPE', 3], 'country in East Asia']\n",
      "[['Russian', 'GPE', 3], 'East Slavic ethnic group']\n",
      "[['Friers', 'ORGANIZATION', 3], 'German actress']\n",
      "[['Bergin', 'PERSON', 3], 'surname']\n",
      "[['German', 'GPE', 3], 'country at the intersection of Central']\n",
      "[['Ruth', 'PERSON', 3], 'property']\n",
      "[['Kolk', 'PERSON', 3], 'psychiatrist']\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def nearestTerm(term, options):\n",
    "    maxRatio = -1.0\n",
    "    nearest = options[0]\n",
    "    for opt in options:\n",
    "        # Use longest contiguous matching subsequence\n",
    "        ratio = SequenceMatcher(None, term, opt).ratio()\n",
    "        if ratio > maxRatio:\n",
    "            maxRatio = ratio\n",
    "            nearest = opt\n",
    "    return nearest\n",
    "\n",
    "def classFromSummary(summary, generic):\n",
    "    first_sent = nltk.sent_tokenize(summary)[0]\n",
    "    first_sent_tokens = nltk.word_tokenize(first_sent)\n",
    "    first_sent_tagged = nltk.pos_tag(first_sent_tokens)\n",
    "#     print(first_sent)\n",
    "#     print(first_sent_tagged)\n",
    "    grammar = \"\"\"\n",
    "        WIKI: {<VBZ|VBP|VBD><DT><JJ|JJR|JJS|CC|NNP|IN|,>*<NN|NNS>+(<IN><DT>?<JJ|JJR|JJS|CC|NNP|IN|,>*<NN|NNS|NNP|NNPS>+(<POS|IN>?<NN|NNS|NNP|NNPS>+)?)?}\n",
    "    \"\"\"\n",
    "    c = nltk.RegexpParser(grammar)\n",
    "    classification = extractEntities(c.parse(first_sent_tagged))\n",
    "#     print(classification)\n",
    "    if len(classification) < 1:\n",
    "        grammar = \"WIKI: {<VBZ|VBP|VBD><JJ|JJR|JJS|CC|NNP|IN|,>*<NN|NNS>+(<IN><DT>?<JJ|JJR|JJS|CC|NNP|IN|,>*<NN|NNS|NNP|NNPS>+(<POS|IN>?<NN|NNS|NNP|NNPS>+)?)?}\"\n",
    "        c = nltk.RegexpParser(grammar)\n",
    "        classification = extractEntities(c.parse(first_sent_tagged))\n",
    "        if len(classification) < 1:\n",
    "            return generic\n",
    "    classification = next(iter(classification))\n",
    "#     print(classification)\n",
    "    classification_tagged = nltk.pos_tag(nltk.word_tokenize(classification))\n",
    "    grammar = \"WIKI: {<JJ|JJR|JJS|CC|NNP|IN|,>*<NN|NNS>+(<IN><DT>?<JJ|JJR|JJS|CC|NNP|IN|,>*<NN|NNS|NNP|NNPS>+(<POS|IN>?<NN|NNS|NNP|NNPS>+)?)?}\"\n",
    "    c = nltk.RegexpParser(grammar)\n",
    "    classification = extractEntities(c.parse(classification_tagged))\n",
    "    classification = next(iter(classification))\n",
    "    return classification\n",
    "\n",
    "def wikipediaClassification(entity):\n",
    "    generic_class = 'generic'\n",
    "    try:\n",
    "        results = wikipedia.search(entity)\n",
    "        if len(results) < 1:\n",
    "            return generic_class\n",
    "        page = wikipedia.page(results[0])\n",
    "        return classFromSummary(page.summary, generic_class)\n",
    "    except wikipedia.DisambiguationError as e:\n",
    "        # Handle ambiguous search result - find nearest option\n",
    "        nearest = nearestTerm(entity, e.options)\n",
    "#         print(nearest)\n",
    "        try:\n",
    "            page = wikipedia.page(nearest)\n",
    "            return classFromSummary(page.summary, generic_class)\n",
    "        except (wikipedia.DisambiguationError, wikipedia.PageError):\n",
    "            # In case of multiple ambiguousity, return generic classification\n",
    "            return generic_class\n",
    "    except wikipedia.PageError:\n",
    "        return generic_class\n",
    "\n",
    "ne_wikipedia_classified = []\n",
    "    \n",
    "for entity in ne_cnts[:50]:\n",
    "    classification = wikipediaClassification(entity[0])\n",
    "    ne_wikipedia_classified.append([entity, classification])\n",
    "    print([entity, classification])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER with custom patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Ireland', 'NP', 36], 'piece of subcontinental land']\n",
      "[['Halligen', 'NP', 27], 'small islands without protective dikes']\n",
      "[['Olkaria', 'NP', 24], 'lies']\n",
      "[['Kenya', 'NP', 14], 'generic']\n",
      "[['Earth', 'NP', 13], 'generic']\n",
      "[['Atlantic', 'NP', 11], \"second-largest of the world 's oceans\"]\n",
      "[['Newson', 'NP', 10], 'generic']\n",
      "[['Hartwig-Kruse', 'NP', 10], 'German politician']\n",
      "[['Karingithi', 'NP', 9], 'generic']\n",
      "[['Daltun', 'NP', 9], 'Goidelic language of the Insular Celtic branch']\n",
      "[['Hooge', 'NP', 9], 'generic']\n",
      "[['Mexico', 'NP', 9], 'country in the southern portion of North America']\n",
      "[['Mwangi', 'NP', 8], 'Kenyan photojournalist']\n",
      "[['Germany', 'NP', 8], 'country at the intersection of Central']\n",
      "[['Nordstrandischmoor', 'NP', 8], 'generic']\n",
      "[['Africa', 'NP', 7], 'country']\n",
      "[['Reid', 'NP', 7], 'technique wherein']\n",
      "[['Maasai', 'NP', 6], 'Nilotic ethnic group']\n",
      "[['Iceland', 'NP', 6], 'island in the North Atlantic']\n",
      "[['Arctic', 'NP', 6], 'generic']\n",
      "[['Doré', 'NP', 6], 'generic']\n",
      "[['Republic', 'NP', 6], 'form of government']\n",
      "[['Deicke', 'NP', 6], 'generic']\n",
      "[['Allen', 'NP', 6], 'English actor']\n",
      "[['Morrison', 'NP', 6], 'fourth largest chain of supermarkets']\n",
      "[['Hell', 'NP', 5], 'generic']\n",
      "[['Gate', 'NP', 5], 'structured form of play']\n",
      "[['CO2', 'NP', 5], 'colorless gas with a density']\n",
      "[['Fogarty', 'NP', 5], 'surname of Irish origin']\n",
      "[[\"O'Connell\", 'NP', 5], 'American singer-songwriter']\n",
      "[['Hallig', 'NP', 5], 'small islands without protective dikes']\n",
      "[['Rösner', 'NP', 5], 'hostage-taking crisis']\n",
      "[['Caribbean', 'NP', 5], 'region of the Americas']\n",
      "[['Wang', 'NP', 5], 'type of fin']\n",
      "[['KenGen', 'NP', 4], 'parastatal company']\n",
      "[['Geothermal', 'NP', 4], 'thermal energy']\n",
      "[['Nyaga', 'NP', 4], 'generic']\n",
      "[['Canada', 'NP', 4], 'country in the northern part of North America']\n",
      "[['Hansen', 'NP', 4], 'American beverage company']\n",
      "[['Europe', 'NP', 4], 'continent']\n",
      "[['Gröde', 'NP', 4], 'municipality in the district of Nordfriesland']\n",
      "[['Suswa', 'NP', 3], 'shield volcano in the Great Rift Valley , Kenya']\n",
      "[['Sempui', 'NP', 3], 'generic']\n",
      "[['China', 'NP', 3], 'country in East Asia']\n",
      "[['Friers', 'NP', 3], 'German actress']\n",
      "[['Stout', 'NP', 3], 'dark , top-fermented beer with a number of variations']\n",
      "[['Bergin', 'NP', 3], 'surname']\n",
      "[['Schleswig-Holstein', 'NP', 3], 'northernmost']\n",
      "[['Without', 'NP', 3], 'upcoming American action thriller film']\n",
      "[['Kolk', 'NP', 3], 'psychiatrist']\n"
     ]
    }
   ],
   "source": [
    "ne_custom_wikipedia_classified = []\n",
    "for entity in ne_custom_cnts[:50]:\n",
    "    classification = wikipediaClassification(entity[0])\n",
    "    ne_custom_wikipedia_classified.append([entity, classification])\n",
    "    print([entity, classification])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|['Ireland', 'GPE']|['Ireland', 'piece of subcontinental land']|['Ireland', 'NP', 'piece of subcontinental land']\n",
      "\n",
      "|['Halligen', 'PERSON']|['Halligen', 'small islands without protective dikes']|['Halligen', 'NP', 'small islands without protective dikes']\n",
      "\n",
      "|['Olkaria', 'GPE']|['Olkaria', 'lies']|['Olkaria', 'NP', 'lies']\n",
      "\n",
      "|['Kenya', 'PERSON']|['Kenya', 'generic']|['Kenya', 'NP', 'generic']\n",
      "\n",
      "|['Earth', 'PERSON']|['Earth', 'generic']|['Earth', 'NP', 'generic']\n",
      "\n",
      "|['Atlantic', 'ORGANIZATION']|['Atlantic', \"second-largest of the world 's oceans\"]|['Atlantic', 'NP', \"second-largest of the world 's oceans\"]\n",
      "\n",
      "|['Newson', 'ORGANIZATION']|['Newson', 'generic']|['Newson', 'NP', 'generic']\n",
      "\n",
      "|['Karingithi', 'PERSON']|['Karingithi', 'generic']|['Hartwig-Kruse', 'NP', 'German politician']\n",
      "\n",
      "|['Mexico', 'GPE']|['Mexico', 'country in the southern portion of North America']|['Karingithi', 'NP', 'generic']\n",
      "\n",
      "|['Daltun', 'PERSON']|['Daltun', 'Goidelic language of the Insular Celtic branch']|['Daltun', 'NP', 'Goidelic language of the Insular Celtic branch']\n",
      "\n",
      "|['Hooge', 'PERSON']|['Hooge', 'generic']|['Hooge', 'NP', 'generic']\n",
      "\n",
      "|['Mwangi', 'PERSON']|['Mwangi', 'Kenyan photojournalist']|['Mexico', 'NP', 'country in the southern portion of North America']\n",
      "\n",
      "|['County', 'PERSON']|['County', 'distinct territorial body']|['Mwangi', 'NP', 'Kenyan photojournalist']\n",
      "\n",
      "|['Germany', 'GPE']|['Germany', 'country at the intersection of Central']|['Germany', 'NP', 'country at the intersection of Central']\n",
      "\n",
      "|['Nordstrandischmoor', 'GPE']|['Nordstrandischmoor', 'generic']|['Nordstrandischmoor', 'NP', 'generic']\n",
      "\n",
      "|['Africa', 'PERSON']|['Africa', 'country']|['Africa', 'NP', 'country']\n",
      "\n",
      "|['Reid', 'PERSON']|['Reid', 'technique wherein']|['Reid', 'NP', 'technique wherein']\n",
      "\n",
      "|['Maasai', 'PERSON']|['Maasai', 'Nilotic ethnic group']|['Maasai', 'NP', 'Nilotic ethnic group']\n",
      "\n",
      "|['Iceland', 'GPE']|['Iceland', 'island in the North Atlantic']|['Iceland', 'NP', 'island in the North Atlantic']\n",
      "\n",
      "|['Arctic', 'ORGANIZATION']|['Arctic', 'generic']|['Arctic', 'NP', 'generic']\n",
      "\n",
      "|['Doré', 'PERSON']|['Doré', 'generic']|['Doré', 'NP', 'generic']\n",
      "\n",
      "|['Irish', 'GPE']|['Irish', 'islands of Ireland and Great Britain']|['Republic', 'NP', 'form of government']\n",
      "\n",
      "|['Deicke', 'PERSON']|['Deicke', 'generic']|['Deicke', 'NP', 'generic']\n",
      "\n",
      "|['Allen', 'PERSON']|['Allen', 'English actor']|['Allen', 'NP', 'English actor']\n",
      "\n",
      "|['Morrison', 'PERSON']|['Morrison', 'fourth largest chain of supermarkets']|['Morrison', 'NP', 'fourth largest chain of supermarkets']\n",
      "\n",
      "|['Hell', 'PERSON']|['Hell', 'generic']|['Hell', 'NP', 'generic']\n",
      "\n",
      "|['Gate', 'ORGANIZATION']|['Gate', 'structured form of play']|['Gate', 'NP', 'structured form of play']\n",
      "\n",
      "|['CO2', 'ORGANIZATION']|['CO2', 'colorless gas with a density']|['CO2', 'NP', 'colorless gas with a density']\n",
      "\n",
      "|['Fogarty', 'PERSON']|['Fogarty', 'surname of Irish origin']|['Fogarty', 'NP', 'surname of Irish origin']\n",
      "\n",
      "|['Hallig', 'PERSON']|['Hallig', 'small islands without protective dikes']|[\"O'Connell\", 'NP', 'American singer-songwriter']\n",
      "\n",
      "|['Rösner', 'PERSON']|['Rösner', 'hostage-taking crisis']|['Hallig', 'NP', 'small islands without protective dikes']\n",
      "\n",
      "|['Caribbean', 'LOCATION']|['Caribbean', 'region of the Americas']|['Rösner', 'NP', 'hostage-taking crisis']\n",
      "\n",
      "|['Wang', 'PERSON']|['Wang', 'type of fin']|['Caribbean', 'NP', 'region of the Americas']\n",
      "\n",
      "|['KenGen', 'ORGANIZATION']|['KenGen', 'parastatal company']|['Wang', 'NP', 'type of fin']\n",
      "\n",
      "|['Geothermal', 'ORGANIZATION']|['Geothermal', 'thermal energy']|['KenGen', 'NP', 'parastatal company']\n",
      "\n",
      "|['Nyaga', 'PERSON']|['Nyaga', 'generic']|['Geothermal', 'NP', 'thermal energy']\n",
      "\n",
      "|['Canada', 'GPE']|['Canada', 'country in the northern part of North America']|['Nyaga', 'NP', 'generic']\n",
      "\n",
      "|['Highway', 'PERSON']|['Highway', 'public or private road']|['Canada', 'NP', 'country in the northern part of North America']\n",
      "\n",
      "|['Hansen', 'PERSON']|['Hansen', 'American beverage company']|['Hansen', 'NP', 'American beverage company']\n",
      "\n",
      "|['Europe', 'PERSON']|['Europe', 'continent']|['Europe', 'NP', 'continent']\n",
      "\n",
      "|['Gröde', 'PERSON']|['Gröde', 'municipality in the district of Nordfriesland']|['Gröde', 'NP', 'municipality in the district of Nordfriesland']\n",
      "\n",
      "|['Suswa', 'PERSON']|['Suswa', 'shield volcano in the Great Rift Valley , Kenya']|['Suswa', 'NP', 'shield volcano in the Great Rift Valley , Kenya']\n",
      "\n",
      "|['Sempui', 'PERSON']|['Sempui', 'generic']|['Sempui', 'NP', 'generic']\n",
      "\n",
      "|['China', 'GPE']|['China', 'country in East Asia']|['China', 'NP', 'country in East Asia']\n",
      "\n",
      "|['Russian', 'GPE']|['Russian', 'East Slavic ethnic group']|['Friers', 'NP', 'German actress']\n",
      "\n",
      "|['Friers', 'ORGANIZATION']|['Friers', 'German actress']|['Stout', 'NP', 'dark , top-fermented beer with a number of variations']\n",
      "\n",
      "|['Bergin', 'PERSON']|['Bergin', 'surname']|['Bergin', 'NP', 'surname']\n",
      "\n",
      "|['German', 'GPE']|['German', 'country at the intersection of Central']|['Schleswig-Holstein', 'NP', 'northernmost']\n",
      "\n",
      "|['Ruth', 'PERSON']|['Ruth', 'property']|['Without', 'NP', 'upcoming American action thriller film']\n",
      "\n",
      "|['Kolk', 'PERSON']|['Kolk', 'psychiatrist']|['Kolk', 'NP', 'psychiatrist']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print('|{}|{}|{}\\n'.format([ne_cnts[i][0], ne_cnts[i][1]], [ne_wikipedia_classified[i][0][0], ne_wikipedia_classified[i][1]], [ne_custom_wikipedia_classified[i][0][0], ne_custom_wikipedia_classified[i][0][1], ne_custom_wikipedia_classified[i][1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
